{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of images: 10728\n"
     ]
    }
   ],
   "source": [
    "data_path = '/data/examples/yolo_data/train_img/'\n",
    "boxes_file = 'train_boxes.p'\n",
    "\n",
    "# bounding box 的訊息\n",
    "with open(boxes_file, 'rb') as file:\n",
    "    boxes_dict = pickle.load(file)\n",
    "# images 的路徑\n",
    "file_list = os.listdir(data_path)\n",
    "print(\"number of images: {}\".format(len(file_list)))\n",
    "file_list = file_list[:10]\n",
    "\n",
    "images = [cv2.imread(os.path.join(data_path, f))[:, :, ::-1] for f in file_list]\n",
    "boxes =  [boxes_dict[f] for f in file_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: (375, 500, 3)\n",
      "boxes information: [  4 363 180 412 344  14  61  63 308 338]\n"
     ]
    }
   ],
   "source": [
    "print(\"images shape: {}\".format(images[0].shape))\n",
    "print(\"boxes information: {}\".format(boxes[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import 已經寫好的 scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import retrain_yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded.\n",
      "There are 10000 images and boxes\n",
      "Preprocessing data...\n",
      "Done preprocessing!\n",
      "start training\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "9000/9000 [==============================] - 166s 18ms/step - loss: 235.7260 - val_loss: 142.6526\n",
      "Epoch 2/10\n",
      "9000/9000 [==============================] - 165s 18ms/step - loss: 112.7896 - val_loss: 107.9060\n",
      "Epoch 3/10\n",
      "9000/9000 [==============================] - 164s 18ms/step - loss: 91.6158 - val_loss: 93.7384\n",
      "Epoch 4/10\n",
      "9000/9000 [==============================] - 164s 18ms/step - loss: 81.5902 - val_loss: 85.5206\n",
      "Epoch 5/10\n",
      "9000/9000 [==============================] - 165s 18ms/step - loss: 76.1777 - val_loss: 82.1222\n",
      "Epoch 6/10\n",
      "9000/9000 [==============================] - 164s 18ms/step - loss: 73.0439 - val_loss: 79.0640\n",
      "Epoch 7/10\n",
      "9000/9000 [==============================] - 165s 18ms/step - loss: 70.4750 - val_loss: 76.4739\n",
      "Epoch 8/10\n",
      "9000/9000 [==============================] - 166s 18ms/step - loss: 68.5632 - val_loss: 74.5860\n",
      "Epoch 9/10\n",
      "9000/9000 [==============================] - 166s 18ms/step - loss: 67.2880 - val_loss: 74.4396\n",
      "Epoch 10/10\n",
      "9000/9000 [==============================] - 166s 18ms/step - loss: 66.0206 - val_loss: 73.0881\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/30\n",
      "9000/9000 [==============================] - 312s 35ms/step - loss: 73.5957 - val_loss: 157.6476\n",
      "Epoch 2/30\n",
      "9000/9000 [==============================] - 302s 34ms/step - loss: 66.3523 - val_loss: 65.4658\n",
      "Epoch 3/30\n",
      "9000/9000 [==============================] - 298s 33ms/step - loss: 68.9984 - val_loss: 210658.7142\n",
      "Epoch 4/30\n",
      "9000/9000 [==============================] - 295s 33ms/step - loss: 77.2222 - val_loss: 158.0104\n",
      "Epoch 5/30\n",
      "9000/9000 [==============================] - 297s 33ms/step - loss: 73.6908 - val_loss: 73.1389\n",
      "Epoch 6/30\n",
      "9000/9000 [==============================] - 298s 33ms/step - loss: 69.3484 - val_loss: 65.5632\n",
      "Epoch 7/30\n",
      "9000/9000 [==============================] - 296s 33ms/step - loss: 64.0826 - val_loss: 66.0012\n",
      "Epoch 8/30\n",
      "9000/9000 [==============================] - 296s 33ms/step - loss: 61.6183 - val_loss: 81.3460\n",
      "Epoch 9/30\n",
      "1784/9000 [====>.........................] - ETA: 3:45 - loss: 61.8660"
     ]
    }
   ],
   "source": [
    "args = retrain_yolo.argparser.parse_known_args()[0]\n",
    "args.data_path = data_path\n",
    "args.boxes_path = boxes_file\n",
    "args.classes_path = '/data/examples/yolo_data/model_data/pascal_classes.txt'\n",
    "args.epoch_1 = 10\n",
    "args.epoch_2 = 30\n",
    "t = retrain_yolo._main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
